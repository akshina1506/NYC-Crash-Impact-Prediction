{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5098a1a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3317f3a2-a07c-412c-b1ed-f6136a0348f6",
   "metadata": {},
   "source": [
    "### Explanation of the `load_data` Function\n",
    "\n",
    "This function performs a series of data preprocessing steps to clean and prepare crash and vehicle datasets for analysis. Below is a detailed breakdown of the operations:\n",
    "\n",
    "---\n",
    "\n",
    "#### **Function Purpose**\n",
    "- **Inputs**:\n",
    "  - `crash_data_path`: Path to the crashes dataset (CSV file).\n",
    "  - `vehicles_data_path`: Path to the vehicles dataset (CSV file).\n",
    "- **Output**:\n",
    "  - A cleaned DataFrame filtered for the years 2022-2023.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step-by-Step Breakdown**\n",
    "\n",
    "1. **Loading Datasets**:\n",
    "   - The crashes dataset is loaded into a DataFrame (`crashes_df`), and its shape is displayed.\n",
    "   - The vehicles dataset is loaded into a DataFrame (`vehicles_df`), and its shape is displayed.\n",
    "\n",
    "2. **Merging Datasets**:\n",
    "   - The two datasets are merged on the common column `COLLISION_ID` using a left join, resulting in `NYC_crashes_df`.\n",
    "   - Initial information about the merged dataset is printed (shape, first few rows, and summary).\n",
    "\n",
    "3. **Dropping Unnecessary Columns**:\n",
    "   - Columns such as `ZIP CODE`, `LOCATION`, and contributing factor columns are dropped to reduce data complexity.\n",
    "\n",
    "4. **Creating and Adjusting Time Features**:\n",
    "   - A new column `YEAR` is created by extracting the year from the `CRASH_DATE` column.\n",
    "   - `CRASH_DATE` and `CRASH_TIME` columns are dropped, as they are no longer needed.\n",
    "\n",
    "5. **Filtering for Specific Years**:\n",
    "   - The dataset is filtered to include only rows corresponding to the years 2022 and 2023.\n",
    "\n",
    "6. **Removing Duplicates**:\n",
    "   - Duplicate rows are removed, and the shape before and after this operation is printed.\n",
    "\n",
    "7. **Handling Missing Values**:\n",
    "   - Missing values in **text columns** (e.g., `BOROUGH`, `VEHICLE TYPE CODE`) are replaced with `'unknown'`.\n",
    "   - Missing values in **numeric columns** (e.g., `LATITUDE`, `NUMBER OF PERSONS INJURED`) are replaced with `0`.\n",
    "   - Remaining null counts per column are checked and displayed.\n",
    "\n",
    "8. **Final Output**:\n",
    "   - The cleaned DataFrame is returned, ready for analysis.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Key Insights**\n",
    "- The function ensures the data is well-prepared by addressing:\n",
    "  - Duplicates and missing values.\n",
    "  - Irrelevant columns for the analysis.\n",
    "  - Filtering only relevant timeframes.\n",
    "- Missing values are systematically handled, improving the robustness of subsequent analyses or modeling steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf50536d-7aa3-44f1-a6d4-5bf7cfef97f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(crash_data_path, vehicles_data_path):\n",
    "    \"\"\"\n",
    "    Function to load the crashes and vehicles datasets, merge them on COLLISION_ID,\n",
    "    create a YEAR column, filter data for 2022-2023, handle duplicates, and manage missing values.\n",
    "\n",
    "    Parameters:\n",
    "    crash_data_path (str): Path to the crashes dataset file.\n",
    "    vehicles_data_path (str): Path to the vehicles dataset file.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The cleaned DataFrame for years 2022-2023.\n",
    "    \"\"\"\n",
    "    # Load the crashes dataset\n",
    "    print(\"******************Loading crash data**************************\\n\")\n",
    "    crashes_df = pd.read_csv(crash_data_path)\n",
    "    print(\"Crashes DataFrame Loaded. Shape of crashes_df:\", crashes_df.shape)\n",
    "\n",
    "    # Load the vehicles dataset\n",
    "    print(\"******************Loading vehicles data**************************\\n\")\n",
    "    vehicles_df = pd.read_csv(vehicles_data_path)\n",
    "    print(\"Vehicles DataFrame Loaded. Shape of vehicles_df:\", vehicles_df.shape)\n",
    "    \n",
    "\n",
    "    # Merge crashes and vehicles datasets on COLLISION_ID\n",
    "    print(\"******************Merging data**************************\\n\")\n",
    "    NYC_crashes_df = vehicles_df.merge(crashes_df, on='COLLISION_ID', how='left')\n",
    "    \n",
    "    print(\"\\n*************** Data Summary Before Cleaning **************\\n\")\n",
    "    print(\"Shape of merged DataFrame (NYC_crashes_df):\", NYC_crashes_df.shape, \"\\n\")\n",
    "    print(NYC_crashes_df.head(3), \"\\n\")\n",
    "    print(NYC_crashes_df.info(), \"\\n\")\n",
    "    \n",
    "    # Drop specified columns\n",
    "    columns_to_drop = ['ZIP CODE', 'LOCATION', 'CONTRIBUTING_FACTOR_1', 'CONTRIBUTING_FACTOR_2']\n",
    "    NYC_crashes_df = NYC_crashes_df.drop(columns=columns_to_drop)\n",
    "    print(f\"\\nDropped columns: {columns_to_drop}\\n\")\n",
    "    print(\"Shape after dropping columns:\", NYC_crashes_df.shape)\n",
    "\n",
    "    # Drop duplicates\n",
    "    print(\"\\n*************** Removing Duplicates **************\\n\")\n",
    "    initial_shape = NYC_crashes_df.shape\n",
    "    NYC_crashes_df = NYC_crashes_df.drop_duplicates()\n",
    "    print(f\"Removed duplicates. Shape changed from {initial_shape} to {NYC_crashes_df.shape}\")\n",
    "\n",
    "    # Handle missing values\n",
    "    print(\"\\n*************** Managing Missing Values **************\\n\")\n",
    "    print(NYC_crashes_df.isnull().sum())\n",
    "\n",
    "    # Replace missing values in text columns with 'unknown'\n",
    "    text_columns = ['BOROUGH','ON STREET NAME', 'PRE_CRASH', 'POINT_OF_IMPACT', \n",
    "                    'CONTRIBUTING FACTOR VEHICLE 1', 'CONTRIBUTING FACTOR VEHICLE 2', \n",
    "                    'CONTRIBUTING FACTOR VEHICLE 3', 'VEHICLE TYPE CODE 1','VEHICLE TYPE CODE 2', 'VEHICLE TYPE CODE 3',\n",
    "                   'TRAVEL_DIRECTION','PRE_CRASH','POINT_OF_IMPACT', 'VEHICLE_DAMAGE','VEHICLE_DAMAGE_1', 'PUBLIC_PROPERTY_DAMAGE','PUBLIC_PROPERTY_DAMAGE_TYPE']\n",
    "    for col in text_columns:\n",
    "        if col in NYC_crashes_df.columns:\n",
    "            NYC_crashes_df[col] = NYC_crashes_df[col].fillna('unknown')\n",
    "    print(f\"Replaced missing values in text columns with 'unknown'.\")\n",
    "\n",
    "    # Replace missing values in numeric columns with 0\n",
    "    numeric_columns = ['LATITUDE', 'LONGITUDE','NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED', 'VEHICLE_OCCUPANTS']\n",
    "    for col in numeric_columns:\n",
    "        if col in NYC_crashes_df.columns:\n",
    "            NYC_crashes_df[col] = NYC_crashes_df[col].fillna(0)\n",
    "    print(f\"Replaced missing values in numeric columns with 0.\")\n",
    " \n",
    "    # Check for remaining nulls\n",
    "    print(\"\\n*************** Checking for Remaining Nulls **************\\n\")\n",
    "    null_counts = NYC_crashes_df.isnull().sum()\n",
    "    print(\"Null counts per column (after cleaning):\")\n",
    "    print(null_counts)\n",
    "    \n",
    "      \n",
    "\n",
    "    return NYC_crashes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e49550-8829-4a59-a5b0-909a59d5765f",
   "metadata": {},
   "source": [
    "### Explanation of the `preprocess_data` Function\n",
    "\n",
    "This function preprocesses the NYC crash dataset to clean, derive new features, and prepare the data for machine learning models. It transforms raw crash data into a structured and feature-enriched format, making it suitable for analytical and predictive tasks.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Function Purpose**\n",
    "- **Input**:\n",
    "  - `df`: Raw crash dataset (Pandas DataFrame).\n",
    "- **Outputs**:\n",
    "  - A cleaned and feature-enriched DataFrame.\n",
    "  - A preprocessing pipeline ready for ML models.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step-by-Step Breakdown**\n",
    "\n",
    "1. **Creating Target Label (`CRASH_SEVERITY`)**:\n",
    "   - **Purpose**: Classify crash severity based on fatalities, injuries, or property damage.\n",
    "   - **Implementation**:\n",
    "     - Rows with fatalities (`NUMBER OF PERSONS KILLED`), injuries (`NUMBER OF PERSONS INJURED`), or public property damage (`PUBLIC_PROPERTY_DAMAGE` = 'Y') are labeled as `High Impact`.\n",
    "     - All other rows are labeled as `Low Impact`.\n",
    "   - **Output**: A new column `CRASH_SEVERITY` is added, and the class distribution is printed.\n",
    "\n",
    "2. **Handling Date and Time Columns**:\n",
    "   - **Purpose**: Extract date and time features for deeper temporal analysis.\n",
    "   - **Steps**:\n",
    "     - Convert `CRASH DATE` and `CRASH TIME` into datetime format.\n",
    "     - Extract features like `year`, `month`, `day_of_week`, and `hour`.\n",
    "     - Create a binary column `is_weekend` indicating whether the crash occurred on a weekend.\n",
    "\n",
    "3. **Deriving `location_type`**:\n",
    "   - **Purpose**: Identify whether the crash occurred in an urban area or on a highway.\n",
    "   - **Logic**:\n",
    "     - If `BOROUGH` is known and valid, it is classified as `Urban`.\n",
    "     - Otherwise, it is classified as `Highway`.\n",
    "\n",
    "4. **Calculating Total Vehicles Involved**:\n",
    "   - **Purpose**: Summarize the number of vehicles involved in each crash.\n",
    "   - **Implementation**:\n",
    "     - Count the non-missing and non-`unknown` values in vehicle-related columns (`VEHICLE TYPE CODE 1`, `VEHICLE TYPE CODE 2`, `VEHICLE TYPE CODE 3`).\n",
    "\n",
    "5. **Categorizing Time of Day**:\n",
    "   - **Purpose**: Add a temporal dimension by categorizing crashes into time-of-day segments.\n",
    "   - **Categories**:\n",
    "     - `Morning` (5 AM–12 PM)\n",
    "     - `Afternoon` (12 PM–5 PM)\n",
    "     - `Evening` (5 PM–9 PM)\n",
    "     - `Night` (9 PM–5 AM)\n",
    "     - `Unknown` for missing hours.\n",
    "\n",
    "6. **Dropping Irrelevant Columns**:\n",
    "   - **Purpose**: Reduce dataset complexity and redundancy.\n",
    "   - **Implementation**:\n",
    "     - Columns related to unique IDs, collision identifiers, and detailed crash metrics are dropped.\n",
    "     - This ensures a cleaner dataset focused on derived and essential features.\n",
    "\n",
    "7. **Final Output**:\n",
    "   - **Result**:\n",
    "     - The cleaned DataFrame is returned with derived features such as `CRASH_SEVERITY`, `location_type`, `total_vehicles_involved`, and `time_of_day`.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Key Insights**\n",
    "- This preprocessing pipeline transforms raw data into actionable insights by adding features and normalizing values.\n",
    "- The dataset is structured for machine learning tasks with a focus on crash severity prediction.\n",
    "- Temporal and spatial attributes enrich the dataset, enabling robust analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9faa8ec4-c63b-497a-9682-ee525797671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the NYC crash dataset to clean, derive new features, normalize text, \n",
    "    prepare text columns for ML models using TF-IDF and One-Hot Encoding, and define crash severity.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The raw crash dataset.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: The cleaned and feature-enriched DataFrame.\n",
    "    ColumnTransformer: Preprocessing pipeline for use in ML models.\n",
    "    \"\"\"\n",
    "    print(\"*************Creating Target Label*****************\\n\")\n",
    "    # Step 1: Define target label \"CRASH_SEVERITY\"\n",
    "    def classify_severity(row):\n",
    "        if row['NUMBER OF PERSONS KILLED'] >= 1 or row['PUBLIC_PROPERTY_DAMAGE'] == 'Y' or row['NUMBER OF PERSONS INJURED'] >= 1:\n",
    "            return 'High Impact'  # Fatality present or property damage or injury\n",
    "        else:\n",
    "            return 'Low Impact'  # No injuries, no fatalities, no property damage\n",
    "    \n",
    "    # Step 2: Apply the function to classify severity\n",
    "    df['CRASH_SEVERITY'] = df.apply(classify_severity, axis=1)\n",
    "    \n",
    "    # Calculate class distribution\n",
    "    class_distribution = df['CRASH_SEVERITY'].value_counts()\n",
    "\n",
    "    print(\"Class Distribution:\\n\", class_distribution)\n",
    "\n",
    "    # Step 3: Convert CRASH DATE and CRASH TIME to datetime formats\n",
    "    df['CRASH_DATE'] = pd.to_datetime(df['CRASH_DATE'], errors='coerce')\n",
    "    df['CRASH_TIME'] = pd.to_datetime(df['CRASH_TIME'], format='%H:%M', errors='coerce')\n",
    "    \n",
    "    # Step 4: Extract date and time features\n",
    "    print(\"*************Extracting date and time features*****************\\n\")\n",
    "    df['year'] = df['CRASH_DATE'].dt.year\n",
    "    df['month'] = df['CRASH_DATE'].dt.month\n",
    "    df['day_of_week'] = df['CRASH_DATE'].dt.day_name()\n",
    "    df['hour'] = df['CRASH_TIME'].dt.hour\n",
    "    df['is_weekend'] = df['day_of_week'].isin(['Saturday', 'Sunday']).astype(int)\n",
    "    \n",
    "    # Step 5: Derive 'location_type'\n",
    "    print(\"*************Deriving 'Location type' feature*****************\\n\")\n",
    "    def location_type(row):\n",
    "        if row['BOROUGH'] and row['BOROUGH'].lower() != 'unknown':\n",
    "            return 'Urban'\n",
    "        else:\n",
    "            return 'Highway'\n",
    "    df['location_type'] = df.apply(location_type, axis=1)\n",
    "    \n",
    "    # Step 6: Count total vehicles involved\n",
    "    print(\"*************Calculating Number of Vehicles involved in the Crash*****************\\n\")\n",
    "    vehicle_columns = ['VEHICLE TYPE CODE 1', 'VEHICLE TYPE CODE 2', 'VEHICLE TYPE CODE 3']\n",
    "    df['total_vehicles_involved'] = df[vehicle_columns].apply(lambda row: row.str.lower().ne('unknown').sum(), axis=1)\n",
    "    \n",
    "    \n",
    "    # Step 7: Categorize time of day\n",
    "    print(\"*************Categorize time of day********************\\n\")\n",
    "    def time_of_day(hour):\n",
    "        if pd.isna(hour):  # Handle missing hour\n",
    "            return 'Unknown'\n",
    "        if 5 <= hour < 12:\n",
    "            return 'Morning'\n",
    "        elif 12 <= hour < 17:\n",
    "            return 'Afternoon'\n",
    "        elif 17 <= hour < 21:\n",
    "            return 'Evening'\n",
    "        else:\n",
    "            return 'Night'\n",
    "    df['time_of_day'] = df['hour'].apply(time_of_day)\n",
    "    \n",
    "\n",
    "    # Step 9: Drop columns\n",
    "    columns_to_drop = ['NUMBER OF PERSONS KILLED', 'NUMBER OF PERSONS INJURED', \n",
    "                   'PUBLIC_PROPERTY_DAMAGE', 'PUBLIC_PROPERTY_DAMAGE_TYPE',\n",
    "                   'VEHICLE TYPE CODE 1', 'VEHICLE TYPE CODE 2', 'VEHICLE TYPE CODE 3', \n",
    "                   'UNIQUE_ID', 'COLLISION_ID']\n",
    "    df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "    \n",
    "    print(f\"\\n processed data :  {df.head(2)} \\n \")\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc81f416-b4f3-402e-9886-d42cdc6f4ff2",
   "metadata": {},
   "source": [
    "### Explanation of the `split_data` Function\n",
    "\n",
    "This function preprocesses the dataset by performing feature transformation and splitting it into training, validation, and test sets. It ensures that the data is ready for machine learning model training and evaluation, using appropriate encoding and scaling techniques for different types of features.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Function Purpose**\n",
    "- **Input**:\n",
    "  - `df`: Preprocessed crash dataset (Pandas DataFrame).\n",
    "- **Outputs**:\n",
    "  - Split and processed features and target data for training, validation, and testing sets (`X_train`, `X_val`, `X_test`, `y_train`, `y_val`, `y_test`).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step-by-Step Breakdown**\n",
    "\n",
    "1. **Splitting Data by Year**:\n",
    "   - **Purpose**: Separate the data into train (2022) and test (2023) sets based on the `year` column.\n",
    "   - **Implementation**:\n",
    "     - `train_data`: All records from 2022.\n",
    "     - `test_data`: All records from 2023.\n",
    "   - **Output**: The dataset is divided into training and test sets for model training and evaluation.\n",
    "\n",
    "2. **Selecting Features and Target**:\n",
    "   - **Purpose**: Define the features and target variable for model training.\n",
    "   - **Implementation**:\n",
    "     - Features (`train_features`) include temporal, spatial, and vehicle-related attributes.\n",
    "     - The target (`y_train_full`) is the crash severity (`CRASH_SEVERITY`).\n",
    "\n",
    "3. **Identifying Categorical and Numeric Columns**:\n",
    "   - **Purpose**: Identify which features need to undergo specific transformations for machine learning models.\n",
    "   - **Categorical columns**: Includes features like `location_type`, `time_of_day`, `TRAVEL_DIRECTION`, etc.\n",
    "   - **Numeric columns**: Includes features like `total_vehicles_involved`, `hour`, and `VEHICLE_OCCUPANTS`.\n",
    "\n",
    "4. **Transforming Categorical Features with One-Hot Encoding**:\n",
    "   - **Purpose**: Convert categorical features into a format suitable for machine learning models.\n",
    "   - **Implementation**:\n",
    "     - Use `OneHotEncoder` to perform one-hot encoding on the categorical columns, with `drop='first'` to avoid multicollinearity.\n",
    "   - **Output**: A matrix of one-hot encoded features for categorical variables.\n",
    "\n",
    "5. **Scaling Numeric Features**:\n",
    "   - **Purpose**: Standardize numeric features to have a mean of 0 and standard deviation of 1.\n",
    "   - **Implementation**:\n",
    "     - Use `StandardScaler` to scale the numeric columns.\n",
    "   - **Output**: A scaled matrix of numeric features.\n",
    "\n",
    "6. **Concatenating Transformed Features**:\n",
    "   - **Purpose**: Combine the transformed categorical and numeric features into a single DataFrame.\n",
    "   - **Implementation**:\n",
    "     - The one-hot encoded categorical features are combined with the scaled numeric features using `pd.concat`.\n",
    "     - Column names are ensured to be consistent by converting all column names to strings.\n",
    "   - **Output**: A processed DataFrame (`X_processed`) containing all features in the correct format for machine learning.\n",
    "\n",
    "7. **Splitting Data into Training, Validation, and Test Sets**:\n",
    "   - **Purpose**: Split the data into three subsets: training, validation, and test.\n",
    "   - **Implementation**:\n",
    "     - Use `train_test_split` to first create an 80% training and 20% temporary set.\n",
    "     - Split the temporary set into 50% validation and 50% test, ensuring stratified sampling based on the target variable (`CRASH_SEVERITY`).\n",
    "   - **Output**: The final split data: `X_train`, `X_val`, `X_test`, `y_train`, `y_val`, and `y_test`.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Key Insights**\n",
    "- **Stratified Sampling**: Ensures that the distribution of crash severity (`CRASH_SEVERITY`) is similar across the training, validation, and test sets, which is critical for balanced model performance.\n",
    "- **One-Hot Encoding and Scaling**: Prepares the data for machine learning models by transforming categorical variables and scaling numeric features.\n",
    "- **Feature Engineering**: Combines various features (temporal, spatial, vehicle-related) to create a rich set of input features for model training.\n",
    "\n",
    "--- \n",
    "\n",
    "#### **Final Output**\n",
    "- The function returns the training, validation, and test sets for both features (`X_train`, `X_val`, `X_test`) and target labels (`y_train`, `y_val`, `y_test`), ready for model development and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbbf2272-35ad-45c0-87c5-c7952da96a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the data and split into train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    # Split train (2022) and test (2023) data\n",
    "    train_data = df[df['year'] == 2022]\n",
    "    test_data = df[df['year'] == 2023]\n",
    "    \n",
    "    print(df.dtypes)\n",
    "\n",
    "    # Train features and target\n",
    "    train_features = ['location_type', 'total_vehicles_involved', 'time_of_day', 'is_weekend',\n",
    "                      'hour', 'day_of_week', 'month', 'year',\n",
    "                      'CONTRIBUTING FACTOR VEHICLE 1',\n",
    "                      'CONTRIBUTING FACTOR VEHICLE 2', 'CONTRIBUTING FACTOR VEHICLE 3',\n",
    "                      'VEHICLE_DAMAGE', 'VEHICLE_DAMAGE_1', 'TRAVEL_DIRECTION', 'VEHICLE_OCCUPANTS', 'PRE_CRASH']\n",
    "    \n",
    "    X_train_full = train_data[train_features]\n",
    "    y_train_full = train_data['CRASH_SEVERITY']\n",
    "\n",
    "    # Step 1: Identify columns\n",
    "    categorical_columns = ['location_type', 'is_weekend', 'day_of_week', 'month', 'TRAVEL_DIRECTION', 'time_of_day',\n",
    "                          'CONTRIBUTING FACTOR VEHICLE 1','CONTRIBUTING FACTOR VEHICLE 2', 'CONTRIBUTING FACTOR VEHICLE 3',\n",
    "                    'VEHICLE_DAMAGE', 'VEHICLE_DAMAGE_1', 'PRE_CRASH']\n",
    "    numeric_columns = ['total_vehicles_involved', 'hour', 'VEHICLE_OCCUPANTS']\n",
    "\n",
    "    # Step 2: One-hot encoding for categorical columns\n",
    "    categorical_transformer = OneHotEncoder(drop='first', sparse_output=False)\n",
    "    X_cat = categorical_transformer.fit_transform(X_train_full[categorical_columns])\n",
    "\n",
    "    \n",
    "    # Step 4: Standard scaling for numeric columns\n",
    "    numeric_transformer = StandardScaler()\n",
    "    X_num = numeric_transformer.fit_transform(X_train_full[numeric_columns])\n",
    "\n",
    "    # Step 5: Concatenate all transformed features\n",
    "    X_cat_df = pd.DataFrame(X_cat, columns=categorical_transformer.get_feature_names_out(categorical_columns))\n",
    "    X_num_df = pd.DataFrame(X_num, columns=numeric_columns)\n",
    "\n",
    "    # Step 6: Concatenate all transformed columns into a single DataFrame\n",
    "    X_processed = pd.concat([X_cat_df, X_num_df], axis=1)\n",
    "\n",
    "    # Ensure all column names are strings (optional but useful for consistency)\n",
    "    X_processed.columns = X_processed.columns.astype(str)\n",
    "\n",
    "    # Check the transformed data\n",
    "    print(X_processed.head())\n",
    "\n",
    "    # Split the data into training, validation, and test sets\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_processed, y_train_full, test_size=0.2, random_state=5, stratify=y_train_full)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=5, stratify=y_temp)\n",
    "    \n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb1b4fc-3ce7-46c4-8160-b45bb09b0db0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. `evaluate_model` Function\n",
    "This function computes and displays a confusion matrix for evaluating classification models. It helps visualize the model's performance by comparing predicted vs. actual class labels.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `y_pred`: The predicted class labels from the model.\n",
    "  - `y_true`: The actual class labels.\n",
    "  - `class_names` (optional): Names of classes for better labeling.\n",
    "  \n",
    "- **Returns**:\n",
    "  - The confusion matrix as a NumPy array.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. `fit_model` Function\n",
    "This function trains and evaluates multiple classification models (Decision Tree, Random Forest) with hyperparameter optimization. It uses `GridSearchCV` for tuning and measures performance metrics such as accuracy, precision, recall, and F1 score.\n",
    "\n",
    "### Steps:\n",
    "1. **Decision Tree Model**:\n",
    "   - Hyperparameter tuning using `GridSearchCV` with parameters like `max_depth` and `min_samples_split`.\n",
    "   - Validation metrics (accuracy, precision, recall, F1 score) are calculated and stored.\n",
    "  \n",
    "2. **Random Forest Model**:\n",
    "   - Optimized hyperparameter tuning with fewer combinations and 3-fold cross-validation to reduce runtime.\n",
    "   - Uses `n_jobs=-1` for parallel processing.\n",
    "   - Validation metrics are calculated and stored.\n",
    "\n",
    "3. **Model Evaluation**:\n",
    "   - For each model, the function:\n",
    "     - Prints key validation metrics.\n",
    "     - Displays the classification report (detailed performance per class).\n",
    "     - Generates and displays the confusion matrix using the `evaluate_model` function.\n",
    "\n",
    "---\n",
    "\n",
    "### **Parameters**:\n",
    "- `X_train`, `y_train`: Training dataset.\n",
    "- `X_val`, `y_val`: Validation dataset.\n",
    "\n",
    "### **Returns**:\n",
    "A dictionary containing:\n",
    "- Fitted models (`Decision Tree` and `Random Forest`).\n",
    "- Their validation metrics (accuracy, precision, recall, F1 score).\n",
    "- Predictions on the validation set.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features and Optimizations\n",
    "\n",
    "### **1. Hyperparameter Tuning**:\n",
    "- Decision Tree and Random Forest models are tuned using `GridSearchCV` with carefully selected parameter ranges.\n",
    "- Reduced parameter combinations and cross-validation folds for faster execution.\n",
    "   \n",
    "### **2. Performance Metrics**:\n",
    "- **Accuracy**: Measures overall correctness.\n",
    "- **Precision, Recall, F1 Score**: Evaluate specific aspects of classification performance.\n",
    "\n",
    "### **3. Parallel Processing**:\n",
    "- For Random Forest, the function leverages parallel processing (`n_jobs=-1`) for quicker hyperparameter tuning.\n",
    "\n",
    "### **4. Confusion Matrix and Reports**:\n",
    "- The `evaluate_model` function visualizes performance and provides insights into class-wise predictions.\n",
    "- The classification report summarizes precision, recall, and F1 scores for each class.\n",
    "\n",
    "### **5. Modularity**:\n",
    "- Each model is trained, validated, and evaluated independently, making the function modular and extensible.\n",
    "\n",
    "---\n",
    "\n",
    "## Purpose\n",
    "This function is ideal for testing multiple models in a streamlined manner while evaluating their performance on validation data. It balances thoroughness (via hyperparameter tuning) with efficiency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d6a0ada-1b58-4939-873f-f67700af3ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_pred, y_true, class_names=None):\n",
    "    \"\"\"\n",
    "    Generates and displays a confusion matrix with a heatmap for better visualization.\n",
    "    \n",
    "    Parameters:\n",
    "    y_pred (array-like): Predicted labels from the model.\n",
    "    y_true (array-like): Actual labels from the data.\n",
    "    class_names (list, optional): List of class names for labeling the matrix.\n",
    "    \n",
    "    Returns:\n",
    "    cm (ndarray): The confusion matrix.\n",
    "    \"\"\"\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    return cm\n",
    "\n",
    "def fit_model(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Fits a Decision Tree, Random Forest, and Gradient Boosting model sequentially \n",
    "    to the input data, and applies hyperparameter tuning to improve recall.\n",
    "\n",
    "    This function takes in the feature matrix (X), the target variable (y), \n",
    "    builds the models, tunes them, and returns the fitted models along with their \n",
    "    performance metrics on the validation set.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Feature matrix for training.\n",
    "        y_train (pd.Series): Target variable for training.\n",
    "        X_val (pd.DataFrame): Feature matrix for validation.\n",
    "        y_val (pd.Series): Target variable for validation.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the fitted models and their respective performance metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    models_results = {}\n",
    "    print(\"\\n ******************running decision tree model*************************\\n\")\n",
    "    # Decision Tree Model\n",
    "    dt_params = {'max_depth': [3, 5, 10, None], 'min_samples_split': [2, 5, 10]}\n",
    "    dt_model = DecisionTreeClassifier(random_state=42)\n",
    "    dt_grid_search = GridSearchCV(dt_model, dt_params, scoring='recall_weighted', cv=5)\n",
    "    dt_grid_search.fit(X_train, y_train)\n",
    "    dt_best_model = dt_grid_search.best_estimator_\n",
    "    \n",
    "    # Validate the Decision Tree model\n",
    "    dt_y_val_pred = dt_best_model.predict(X_val)\n",
    "    dt_accuracy = accuracy_score(y_val, dt_y_val_pred)\n",
    "    dt_precision = precision_score(y_val, dt_y_val_pred, average='weighted')\n",
    "    dt_recall = recall_score(y_val, dt_y_val_pred, average='weighted')\n",
    "    dt_f1 = f1_score(y_val, dt_y_val_pred, average='weighted')\n",
    "\n",
    "    # Store results\n",
    "    models_results['Decision Tree'] = {\n",
    "        'model': dt_best_model, 'accuracy': dt_accuracy, 'precision': dt_precision,\n",
    "        'recall': dt_recall, 'f1': dt_f1, 'y_val_pred': dt_y_val_pred\n",
    "    }\n",
    "    print(\"\\n ******************running Random Forest model*************************\\n\")\n",
    "    # Optimized Random Forest hyperparameters\n",
    "    rf_params = {\n",
    "        'n_estimators': [50, 100],  # Fewer estimators to speed up training\n",
    "        'max_depth': [5, 10],  # Limited max depth\n",
    "        'min_samples_split': [2, 5]  # Fewer combinations\n",
    "    }\n",
    "\n",
    "    # Initialize the Random Forest model\n",
    "    rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    # GridSearchCV with fewer cross-validation folds and parallel processing\n",
    "    rf_grid_search = GridSearchCV(\n",
    "        rf_model, rf_params, scoring='recall_weighted', cv=3, n_jobs=-1  # Use 3-fold CV and parallel processing\n",
    "    )\n",
    "\n",
    "    # Fit the model\n",
    "    rf_grid_search.fit(X_train, y_train)\n",
    "    rf_best_model = rf_grid_search.best_estimator_\n",
    "\n",
    "    # Validate the Random Forest model\n",
    "    rf_y_val_pred = rf_best_model.predict(X_val)\n",
    "    rf_accuracy = accuracy_score(y_val, rf_y_val_pred)\n",
    "    rf_precision = precision_score(y_val, rf_y_val_pred, average='weighted')\n",
    "    rf_recall = recall_score(y_val, rf_y_val_pred, average='weighted')\n",
    "    rf_f1 = f1_score(y_val, rf_y_val_pred, average='weighted')\n",
    "\n",
    "    # Store results\n",
    "    models_results['Random Forest'] = {\n",
    "        'model': rf_best_model, 'accuracy': rf_accuracy, 'precision': rf_precision,\n",
    "        'recall': rf_recall, 'f1': rf_f1, 'y_val_pred': rf_y_val_pred\n",
    "    }\n",
    "   \n",
    "\n",
    "    print(\"\\n ***************Display classification report and confusion matrix for each model*****************\\n\")\n",
    "    # Display classification report and confusion matrix for each model\n",
    "    for model_name, model_data in models_results.items():\n",
    "        print(f\"\\nModel: {model_name}\")\n",
    "        print(f\"Validation Accuracy: {model_data['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {model_data['precision']:.4f}\")\n",
    "        print(f\"Recall: {model_data['recall']:.4f}\")\n",
    "        print(f\"F1 Score: {model_data['f1']:.4f}\\n\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_val, model_data['y_val_pred']))\n",
    "\n",
    "        # Confusion Matrix\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        cm = evaluate_model(model_data['y_val_pred'], y_val)\n",
    "    \n",
    "    return models_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4b1036-e395-4754-806e-a753d35f2ec4",
   "metadata": {},
   "source": [
    "### Running the main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7625f4e-33d2-4d63-ac61-145fa53af072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Step 1: Load the data using the `load_data` function\n",
    "    crash_data_path = \"crashes_data.csv\" \n",
    "    vehicles_data_path = \"vehicles_data.csv\"\n",
    "    \n",
    "    # Load and clean the data\n",
    "    print(\"\\n*************** Loading Data ***************\\n\")\n",
    "    NYC_crashes_df = load_data(crash_data_path, vehicles_data_path)\n",
    "    print(\"\\n*************** Loaded and Cleaned Data ***************\\n\")\n",
    "    print(NYC_crashes_df.head(3))\n",
    "    print(f\"Shape of the cleaned DataFrame: {NYC_crashes_df.shape}\\n\")\n",
    "    \n",
    "    # Step 2: Preprocess the data\n",
    "    print(\"\\n*************** Preprocessing Data ***************\\n\")\n",
    "    NYC_crashes_df = preprocess_data(NYC_crashes_df)\n",
    "    if NYC_crashes_df is None:\n",
    "        raise ValueError(\"Data preprocessing failed. The returned DataFrame is None.\")\n",
    "    print(\"Preprocessed data shape: \", NYC_crashes_df.shape)\n",
    "    \n",
    "    # Step 3: Split the data\n",
    "    print(\"\\n*************** Splitting Data ***************\\n\")\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(NYC_crashes_df)\n",
    "        \n",
    "    # Step 4: Fit and evaluate the model\n",
    "    print(\"\\n*************** Fitting and Evaluating Models ***************\\n\")\n",
    "    models_results = fit_model(X_train, y_train, X_val, y_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a981d5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*************** Loading Data ***************\n",
      "\n",
      "******************Loading crash data**************************\n",
      "\n",
      "Crashes DataFrame Loaded. Shape of crashes_df: (200489, 16)\n",
      "******************Loading vehicles data**************************\n",
      "\n",
      "Vehicles DataFrame Loaded. Shape of vehicles_df: (405570, 15)\n",
      "******************Merging data**************************\n",
      "\n",
      "\n",
      "*************** Data Summary Before Cleaning **************\n",
      "\n",
      "Shape of merged DataFrame (NYC_crashes_df): (405570, 30) \n",
      "\n",
      "   UNIQUE_ID  COLLISION_ID  CRASH_DATE CRASH_TIME TRAVEL_DIRECTION  \\\n",
      "0   20404266       4612832  03/09/2023      20:38             West   \n",
      "1   20393371       4607823  02/20/2023       7:00            North   \n",
      "2   20338184       4582398  11/08/2022      19:03            North   \n",
      "\n",
      "   VEHICLE_OCCUPANTS             PRE_CRASH            POINT_OF_IMPACT  \\\n",
      "0                1.0                Parked           Center Front End   \n",
      "1                1.0    Stopped in Traffic  Right Front Quarter Panel   \n",
      "2                1.0  Going Straight Ahead  Right Front Quarter Panel   \n",
      "\n",
      "              VEHICLE_DAMAGE   VEHICLE_DAMAGE_1  ...      ON STREET NAME  \\\n",
      "0           Center Front End  Left Front Bumper  ...                 NaN   \n",
      "1         Right Front Bumper                NaN  ...     WEST 110 STREET   \n",
      "2  Right Front Quarter Panel                NaN  ...  MARBLE HILL AVENUE   \n",
      "\n",
      "  NUMBER OF PERSONS INJURED NUMBER OF PERSONS KILLED  \\\n",
      "0                       0.0                      0.0   \n",
      "1                       0.0                      0.0   \n",
      "2                       1.0                      0.0   \n",
      "\n",
      "    CONTRIBUTING FACTOR VEHICLE 1  CONTRIBUTING FACTOR VEHICLE 2  \\\n",
      "0                Backing Unsafely                    Unspecified   \n",
      "1  Passing or Lane Usage Improper                    Unspecified   \n",
      "2                     Unspecified                    Unspecified   \n",
      "\n",
      "  CONTRIBUTING FACTOR VEHICLE 3                  VEHICLE TYPE CODE 1  \\\n",
      "0                           NaN                        Pick-up Truck   \n",
      "1                           NaN  Station Wagon/Sport Utility Vehicle   \n",
      "2                           NaN                                Sedan   \n",
      "\n",
      "   VEHICLE TYPE CODE 2  VEHICLE TYPE CODE 3 YEAR_y  \n",
      "0                Sedan                  NaN   2023  \n",
      "1                Sedan                  NaN   2023  \n",
      "2                 Bike                  NaN   2022  \n",
      "\n",
      "[3 rows x 30 columns] \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 405570 entries, 0 to 405569\n",
      "Data columns (total 30 columns):\n",
      " #   Column                         Non-Null Count   Dtype  \n",
      "---  ------                         --------------   -----  \n",
      " 0   UNIQUE_ID                      405570 non-null  int64  \n",
      " 1   COLLISION_ID                   405570 non-null  int64  \n",
      " 2   CRASH_DATE                     405570 non-null  object \n",
      " 3   CRASH_TIME                     405570 non-null  object \n",
      " 4   TRAVEL_DIRECTION               377229 non-null  object \n",
      " 5   VEHICLE_OCCUPANTS              351204 non-null  float64\n",
      " 6   PRE_CRASH                      372883 non-null  object \n",
      " 7   POINT_OF_IMPACT                372545 non-null  object \n",
      " 8   VEHICLE_DAMAGE                 366254 non-null  object \n",
      " 9   VEHICLE_DAMAGE_1               250071 non-null  object \n",
      " 10  PUBLIC_PROPERTY_DAMAGE         405570 non-null  object \n",
      " 11  PUBLIC_PROPERTY_DAMAGE_TYPE    5734 non-null    object \n",
      " 12  CONTRIBUTING_FACTOR_1          379872 non-null  object \n",
      " 13  CONTRIBUTING_FACTOR_2          374100 non-null  object \n",
      " 14  YEAR_x                         405570 non-null  int64  \n",
      " 15  BOROUGH                        270426 non-null  object \n",
      " 16  ZIP CODE                       270377 non-null  float64\n",
      " 17  LATITUDE                       372416 non-null  float64\n",
      " 18  LONGITUDE                      372416 non-null  float64\n",
      " 19  LOCATION                       372416 non-null  object \n",
      " 20  ON STREET NAME                 292517 non-null  object \n",
      " 21  NUMBER OF PERSONS INJURED      405570 non-null  float64\n",
      " 22  NUMBER OF PERSONS KILLED       405570 non-null  float64\n",
      " 23  CONTRIBUTING FACTOR VEHICLE 1  404231 non-null  object \n",
      " 24  CONTRIBUTING FACTOR VEHICLE 2  337732 non-null  object \n",
      " 25  CONTRIBUTING FACTOR VEHICLE 3  65186 non-null   object \n",
      " 26  VEHICLE TYPE CODE 1            402220 non-null  object \n",
      " 27  VEHICLE TYPE CODE 2            295780 non-null  object \n",
      " 28  VEHICLE TYPE CODE 3            61024 non-null   object \n",
      " 29  YEAR_y                         405570 non-null  int64  \n",
      "dtypes: float64(6), int64(4), object(20)\n",
      "memory usage: 95.9+ MB\n",
      "None \n",
      "\n",
      "\n",
      "Dropped columns: ['ZIP CODE', 'LOCATION', 'CONTRIBUTING_FACTOR_1', 'CONTRIBUTING_FACTOR_2']\n",
      "\n",
      "Shape after dropping columns: (405570, 26)\n",
      "\n",
      "*************** Removing Duplicates **************\n",
      "\n",
      "Removed duplicates. Shape changed from (405570, 26) to (405570, 26)\n",
      "\n",
      "*************** Managing Missing Values **************\n",
      "\n",
      "UNIQUE_ID                             0\n",
      "COLLISION_ID                          0\n",
      "CRASH_DATE                            0\n",
      "CRASH_TIME                            0\n",
      "TRAVEL_DIRECTION                  28341\n",
      "VEHICLE_OCCUPANTS                 54366\n",
      "PRE_CRASH                         32687\n",
      "POINT_OF_IMPACT                   33025\n",
      "VEHICLE_DAMAGE                    39316\n",
      "VEHICLE_DAMAGE_1                 155499\n",
      "PUBLIC_PROPERTY_DAMAGE                0\n",
      "PUBLIC_PROPERTY_DAMAGE_TYPE      399836\n",
      "YEAR_x                                0\n",
      "BOROUGH                          135144\n",
      "LATITUDE                          33154\n",
      "LONGITUDE                         33154\n",
      "ON STREET NAME                   113053\n",
      "NUMBER OF PERSONS INJURED             0\n",
      "NUMBER OF PERSONS KILLED              0\n",
      "CONTRIBUTING FACTOR VEHICLE 1      1339\n",
      "CONTRIBUTING FACTOR VEHICLE 2     67838\n",
      "CONTRIBUTING FACTOR VEHICLE 3    340384\n",
      "VEHICLE TYPE CODE 1                3350\n",
      "VEHICLE TYPE CODE 2              109790\n",
      "VEHICLE TYPE CODE 3              344546\n",
      "YEAR_y                                0\n",
      "dtype: int64\n",
      "Replaced missing values in text columns with 'unknown'.\n",
      "Replaced missing values in numeric columns with 0.\n",
      "\n",
      "*************** Checking for Remaining Nulls **************\n",
      "\n",
      "Null counts per column (after cleaning):\n",
      "UNIQUE_ID                        0\n",
      "COLLISION_ID                     0\n",
      "CRASH_DATE                       0\n",
      "CRASH_TIME                       0\n",
      "TRAVEL_DIRECTION                 0\n",
      "VEHICLE_OCCUPANTS                0\n",
      "PRE_CRASH                        0\n",
      "POINT_OF_IMPACT                  0\n",
      "VEHICLE_DAMAGE                   0\n",
      "VEHICLE_DAMAGE_1                 0\n",
      "PUBLIC_PROPERTY_DAMAGE           0\n",
      "PUBLIC_PROPERTY_DAMAGE_TYPE      0\n",
      "YEAR_x                           0\n",
      "BOROUGH                          0\n",
      "LATITUDE                         0\n",
      "LONGITUDE                        0\n",
      "ON STREET NAME                   0\n",
      "NUMBER OF PERSONS INJURED        0\n",
      "NUMBER OF PERSONS KILLED         0\n",
      "CONTRIBUTING FACTOR VEHICLE 1    0\n",
      "CONTRIBUTING FACTOR VEHICLE 2    0\n",
      "CONTRIBUTING FACTOR VEHICLE 3    0\n",
      "VEHICLE TYPE CODE 1              0\n",
      "VEHICLE TYPE CODE 2              0\n",
      "VEHICLE TYPE CODE 3              0\n",
      "YEAR_y                           0\n",
      "dtype: int64\n",
      "\n",
      "*************** Loaded and Cleaned Data ***************\n",
      "\n",
      "   UNIQUE_ID  COLLISION_ID  CRASH_DATE CRASH_TIME TRAVEL_DIRECTION  \\\n",
      "0   20404266       4612832  03/09/2023      20:38             West   \n",
      "1   20393371       4607823  02/20/2023       7:00            North   \n",
      "2   20338184       4582398  11/08/2022      19:03            North   \n",
      "\n",
      "   VEHICLE_OCCUPANTS             PRE_CRASH            POINT_OF_IMPACT  \\\n",
      "0                1.0                Parked           Center Front End   \n",
      "1                1.0    Stopped in Traffic  Right Front Quarter Panel   \n",
      "2                1.0  Going Straight Ahead  Right Front Quarter Panel   \n",
      "\n",
      "              VEHICLE_DAMAGE   VEHICLE_DAMAGE_1  ...      ON STREET NAME  \\\n",
      "0           Center Front End  Left Front Bumper  ...             unknown   \n",
      "1         Right Front Bumper            unknown  ...     WEST 110 STREET   \n",
      "2  Right Front Quarter Panel            unknown  ...  MARBLE HILL AVENUE   \n",
      "\n",
      "  NUMBER OF PERSONS INJURED  NUMBER OF PERSONS KILLED  \\\n",
      "0                       0.0                       0.0   \n",
      "1                       0.0                       0.0   \n",
      "2                       1.0                       0.0   \n",
      "\n",
      "    CONTRIBUTING FACTOR VEHICLE 1  CONTRIBUTING FACTOR VEHICLE 2  \\\n",
      "0                Backing Unsafely                    Unspecified   \n",
      "1  Passing or Lane Usage Improper                    Unspecified   \n",
      "2                     Unspecified                    Unspecified   \n",
      "\n",
      "   CONTRIBUTING FACTOR VEHICLE 3                  VEHICLE TYPE CODE 1  \\\n",
      "0                        unknown                        Pick-up Truck   \n",
      "1                        unknown  Station Wagon/Sport Utility Vehicle   \n",
      "2                        unknown                                Sedan   \n",
      "\n",
      "   VEHICLE TYPE CODE 2  VEHICLE TYPE CODE 3 YEAR_y  \n",
      "0                Sedan              unknown   2023  \n",
      "1                Sedan              unknown   2023  \n",
      "2                 Bike              unknown   2022  \n",
      "\n",
      "[3 rows x 26 columns]\n",
      "Shape of the cleaned DataFrame: (405570, 26)\n",
      "\n",
      "\n",
      "*************** Preprocessing Data ***************\n",
      "\n",
      "*************Creating Target Label*****************\n",
      "\n",
      "Class Distribution:\n",
      " Low Impact     249483\n",
      "High Impact    156087\n",
      "Name: CRASH_SEVERITY, dtype: int64\n",
      "*************Extracting date and time features*****************\n",
      "\n",
      "*************Deriving 'Location type' feature*****************\n",
      "\n",
      "*************Calculating Number of Vehicles involved in the Crash*****************\n",
      "\n",
      "*************Categorize time of day********************\n",
      "\n",
      "\n",
      " processed data :    CRASH_DATE          CRASH_TIME TRAVEL_DIRECTION  VEHICLE_OCCUPANTS  \\\n",
      "0 2023-03-09 1900-01-01 20:38:00             West                1.0   \n",
      "1 2023-02-20 1900-01-01 07:00:00            North                1.0   \n",
      "\n",
      "            PRE_CRASH            POINT_OF_IMPACT      VEHICLE_DAMAGE  \\\n",
      "0              Parked           Center Front End    Center Front End   \n",
      "1  Stopped in Traffic  Right Front Quarter Panel  Right Front Bumper   \n",
      "\n",
      "    VEHICLE_DAMAGE_1  YEAR_x  BOROUGH  ...  YEAR_y  CRASH_SEVERITY  year  \\\n",
      "0  Left Front Bumper    2023    BRONX  ...    2023      Low Impact  2023   \n",
      "1            unknown    2023  unknown  ...    2023      Low Impact  2023   \n",
      "\n",
      "  month day_of_week hour  is_weekend location_type  total_vehicles_involved  \\\n",
      "0     3    Thursday   20           0         Urban                        2   \n",
      "1     2      Monday    7           0       Highway                        2   \n",
      "\n",
      "   time_of_day  \n",
      "0      Evening  \n",
      "1      Morning  \n",
      "\n",
      "[2 rows x 26 columns] \n",
      " \n",
      "Preprocessed data shape:  (405570, 26)\n",
      "\n",
      "*************** Splitting Data ***************\n",
      "\n",
      "CRASH_DATE                       datetime64[ns]\n",
      "CRASH_TIME                       datetime64[ns]\n",
      "TRAVEL_DIRECTION                         object\n",
      "VEHICLE_OCCUPANTS                       float64\n",
      "PRE_CRASH                                object\n",
      "POINT_OF_IMPACT                          object\n",
      "VEHICLE_DAMAGE                           object\n",
      "VEHICLE_DAMAGE_1                         object\n",
      "YEAR_x                                    int64\n",
      "BOROUGH                                  object\n",
      "LATITUDE                                float64\n",
      "LONGITUDE                               float64\n",
      "ON STREET NAME                           object\n",
      "CONTRIBUTING FACTOR VEHICLE 1            object\n",
      "CONTRIBUTING FACTOR VEHICLE 2            object\n",
      "CONTRIBUTING FACTOR VEHICLE 3            object\n",
      "YEAR_y                                    int64\n",
      "CRASH_SEVERITY                           object\n",
      "year                                      int64\n",
      "month                                     int64\n",
      "day_of_week                              object\n",
      "hour                                      int64\n",
      "is_weekend                                int64\n",
      "location_type                            object\n",
      "total_vehicles_involved                   int64\n",
      "time_of_day                              object\n",
      "dtype: object\n",
      "   location_type_Urban  is_weekend_1  day_of_week_Monday  \\\n",
      "0                  1.0           0.0                 0.0   \n",
      "1                  1.0           0.0                 0.0   \n",
      "2                  1.0           1.0                 0.0   \n",
      "3                  1.0           1.0                 0.0   \n",
      "4                  1.0           0.0                 1.0   \n",
      "\n",
      "   day_of_week_Saturday  day_of_week_Sunday  day_of_week_Thursday  \\\n",
      "0                   0.0                 0.0                   0.0   \n",
      "1                   0.0                 0.0                   0.0   \n",
      "2                   1.0                 0.0                   0.0   \n",
      "3                   1.0                 0.0                   0.0   \n",
      "4                   0.0                 0.0                   0.0   \n",
      "\n",
      "   day_of_week_Tuesday  day_of_week_Wednesday  month_2  month_3  ...  \\\n",
      "0                  1.0                    0.0      0.0      0.0  ...   \n",
      "1                  0.0                    1.0      0.0      0.0  ...   \n",
      "2                  0.0                    0.0      0.0      0.0  ...   \n",
      "3                  0.0                    0.0      0.0      0.0  ...   \n",
      "4                  0.0                    0.0      0.0      0.0  ...   \n",
      "\n",
      "   PRE_CRASH_Passing  PRE_CRASH_Police Pursuit  PRE_CRASH_Slowing or Stopping  \\\n",
      "0                0.0                       0.0                            0.0   \n",
      "1                0.0                       0.0                            0.0   \n",
      "2                0.0                       0.0                            0.0   \n",
      "3                0.0                       0.0                            0.0   \n",
      "4                0.0                       0.0                            0.0   \n",
      "\n",
      "   PRE_CRASH_Starting from Parking  PRE_CRASH_Starting in Traffic  \\\n",
      "0                              0.0                            0.0   \n",
      "1                              0.0                            0.0   \n",
      "2                              0.0                            0.0   \n",
      "3                              0.0                            0.0   \n",
      "4                              0.0                            0.0   \n",
      "\n",
      "   PRE_CRASH_Stopped in Traffic  PRE_CRASH_unknown  total_vehicles_involved  \\\n",
      "0                           0.0                0.0                 0.196431   \n",
      "1                           0.0                0.0                 0.196431   \n",
      "2                           0.0                0.0                 0.196431   \n",
      "3                           0.0                0.0                 0.196431   \n",
      "4                           0.0                0.0                -1.322427   \n",
      "\n",
      "       hour  VEHICLE_OCCUPANTS  \n",
      "0  1.007775          -0.001548  \n",
      "1  1.007775          -0.001548  \n",
      "2 -1.982881          -0.020340  \n",
      "3 -1.982881          -0.020340  \n",
      "4 -1.982881          -0.020340  \n",
      "\n",
      "[5 rows x 222 columns]\n",
      "\n",
      "*************** Fitting and Evaluating Models ***************\n",
      "\n",
      "\n",
      " ******************running decision tree model*************************\n",
      "\n",
      "\n",
      " ******************running Random Forest model*************************\n",
      "\n",
      "\n",
      " ***************Display classification report and confusion matrix for each model*****************\n",
      "\n",
      "\n",
      "Model: Decision Tree\n",
      "Validation Accuracy: 0.7035\n",
      "Precision: 0.6935\n",
      "Recall: 0.7035\n",
      "F1 Score: 0.6907\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " High Impact       0.63      0.46      0.53      7706\n",
      "  Low Impact       0.73      0.85      0.78     13355\n",
      "\n",
      "    accuracy                           0.70     21061\n",
      "   macro avg       0.68      0.65      0.66     21061\n",
      "weighted avg       0.69      0.70      0.69     21061\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "Confusion Matrix:\n",
      " [[ 3523  4183]\n",
      " [ 2062 11293]]\n",
      "\n",
      "Model: Random Forest\n",
      "Validation Accuracy: 0.6960\n",
      "Precision: 0.6978\n",
      "Recall: 0.6960\n",
      "F1 Score: 0.6553\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " High Impact       0.70      0.29      0.41      7706\n",
      "  Low Impact       0.69      0.93      0.79     13355\n",
      "\n",
      "    accuracy                           0.70     21061\n",
      "   macro avg       0.70      0.61      0.60     21061\n",
      "weighted avg       0.70      0.70      0.66     21061\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "Confusion Matrix:\n",
      " [[ 2257  5449]\n",
      " [  953 12402]]\n"
     ]
    }
   ],
   "source": [
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18cdd89-d00e-4cbb-85d0-3a5755d8d46b",
   "metadata": {},
   "source": [
    "# Model Review and Analysis\n",
    "\n",
    "### Objective:\n",
    "The model is designed to predict crash severity (either \"Low Impact\" or \"High Impact\") based on various features, including the date and time of the crash, the contributing factors, and the number of vehicles involved.\n",
    "\n",
    "### Key Steps in the Process:\n",
    "\n",
    "#### 1. **Data Preprocessing:**\n",
    "   - The data was filtered to include only crashes from 2022 and 2023, resulting in a cleaned dataset with 405,570 records.\n",
    "   - Missing values were appropriately handled by filling missing text columns with \"unknown\" and numeric columns with 0.\n",
    "   - Unnecessary columns such as \"ZIP CODE\" and \"LOCATION\" were dropped to streamline the dataset.\n",
    "   - Additional columns like `YEAR` were derived from the `CRASH_DATE`, and new features like `location_type` and `total_vehicles_involved` were created.\n",
    "\n",
    "#### 2. **Feature Engineering:**\n",
    "   - Date and time-related features were derived, such as `year`, `month`, `day_of_week`, and `hour`, which may help in identifying patterns based on the time of day, day of the week, or month.\n",
    "   - A new feature `is_weekend` was added to capture whether the crash occurred on a weekend, which could be important for understanding crash frequency.\n",
    "   - The number of vehicles involved was calculated, which likely correlates with the severity of the crash.\n",
    "\n",
    "#### 3. **Handling Categorical Features:**\n",
    "   - Several categorical features,were encoded as dummy variables, allowing the model to process them effectively.\n",
    "\n",
    "---\n",
    "\n",
    "# Model Performance Summary\n",
    "\n",
    "The results of the Decision Tree and Random Forest models were evaluated using validation accuracy, precision, recall, F1 score, classification reports, and confusion matrices. Here’s a summary of the outcomes:\n",
    "\n",
    "---\n",
    "\n",
    "## Decision Tree\n",
    "- **Validation Accuracy:** 70.51%\n",
    "- **Precision:** 69.59%\n",
    "- **Recall:** 70.51%\n",
    "- **F1 Score:** 69.51%\n",
    "\n",
    "### Classification Report:\n",
    "- The Decision Tree model showed better performance on the **\"Low Impact\"** class compared to the **\"High Impact\"** class.\n",
    "  - For **High Impact**, the precision was **63%**, with a recall of **48%**, indicating it struggles to correctly identify instances of this class.\n",
    "  - For **Low Impact**, the precision was **74%**, with a recall of **83%**, demonstrating stronger predictive power for this class.\n",
    "- The overall weighted averages for precision, recall, and F1 score were around **70%**, reflecting balanced but slightly skewed performance.\n",
    "\n",
    "### Confusion Matrix:\n",
    "- **True Positives (High Impact):** 3711  \n",
    "- **False Positives (High Impact):** 3995  \n",
    "- **True Positives (Low Impact):** 11140  \n",
    "- **False Positives (Low Impact):** 2215  \n",
    "\n",
    "---\n",
    "\n",
    "## Random Forest\n",
    "- **Validation Accuracy:** 69.53%\n",
    "- **Precision:** 70.36%\n",
    "- **Recall:** 69.53%\n",
    "- **F1 Score:** 64.86%\n",
    "\n",
    "### Classification Report:\n",
    "- The Random Forest model also demonstrated better performance on the **\"Low Impact\"** class:\n",
    "  - For **High Impact**, precision was **73%**, but recall was significantly lower at **27%**, indicating a high rate of missed classifications.\n",
    "  - For **Low Impact**, precision was **69%**, with a high recall of **94%**, showing strong performance in capturing this class.\n",
    "- The weighted average F1 score was **64.86%**, slightly lower than the Decision Tree model, largely due to imbalanced performance across classes.\n",
    "\n",
    "### Confusion Matrix:\n",
    "- **True Positives (High Impact):** 2068  \n",
    "- **False Positives (High Impact):** 5638  \n",
    "- **True Positives (Low Impact):** 12576  \n",
    "- **False Positives (Low Impact):** 779  \n",
    "\n",
    "---\n",
    "\n",
    "## Key Insights\n",
    "1. Both models performed better at identifying **\"Low Impact\"** instances than **\"High Impact\"**, suggesting class imbalance or inherent difficulty in distinguishing \"High Impact.\"\n",
    "2. The Decision Tree had slightly higher F1 scores for overall performance, particularly benefiting from more balanced recall across classes.\n",
    "3. The Random Forest, while providing better precision for the \"High Impact\" class, struggled with recall for this class, resulting in a large number of misclassifications.\n",
    "4. Future improvements could focus on addressing class imbalance or improving feature engineering to boost performance for **\"High Impact\"** cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c71e146-e338-42c1-959a-2cf569e59e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c53adba-a7a2-4833-aa83-fb0c3637b036",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
